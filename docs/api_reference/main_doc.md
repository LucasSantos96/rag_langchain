# ğŸ“– DocumentaÃ§Ã£o - main.py

## ğŸ¯ VisÃ£o Geral

O arquivo `main.py` implementa a **interface principal de consulta** do sistema RAG, permitindo que usuÃ¡rios faÃ§am perguntas em linguagem natural e recebam respostas baseadas nos documentos processados.

### ğŸ—ï¸ Arquitetura da FunÃ§Ã£o Principal

O sistema implementa o fluxo completo de RAG (Retrieval-Augmented Generation):

```
ğŸ“ Pergunta do UsuÃ¡rio â†’ ğŸ§  Embedding â†’ ğŸ” Busca no ChromaDB â†’ ğŸ“„ Contexto Recuperado â†’ ğŸ¤– LLM â†’ ğŸ’¬ Resposta Final
```

---

## ğŸ“ CÃ³digo Completo e ExplicaÃ§Ã£o

```python
# ğŸ“š Imports NecessÃ¡rios
from langchain_chroma.vectorstores import Chroma
from langchain_openai import OpenAIEmbeddings
from dotenv import load_dotenv
from langchain_core.prompts import ChatPromptTemplate
from langchain_openai import ChatOpenAI

# ğŸ”§ ConfiguraÃ§Ã£o Inicial
load_dotenv()  # Carrega variÃ¡veis de ambiente do .env

CAMINHO_DB = "db"  # DiretÃ³rio do banco vetorial ChromaDB

# ğŸ¤– Template de Prompt Estruturado
prompt_template = """VocÃª Ã© um assistente inteligente que ajuda os usuÃ¡rios com suas perguntas com base em documentos fornecidos:
{pergunta}

Utilize as informaÃ§Ãµes do documentos para responder Ã  pergunta acima. ForneÃ§a respostas detalhadas e precisas:

{base_conhecimento}

Se a informaÃ§Ã£o nÃ£o estiver disponÃ­vel nos documentos, responda com "Desculpe, nÃ£o sei a resposta para essa pergunta."
"""

# ğŸ¯ FunÃ§Ã£o Principal de Consulta RAG
def perguntar():
    pergunta = input("Digite sua pergunta: ")
    
    # ğŸ§  Carregar o banco de dados vetorizado
    func_embedding = OpenAIEmbeddings()
    db = Chroma(persist_directory=CAMINHO_DB, embedding_function=func_embedding)
    
    # ğŸ” Comparar a pergunta com os documentos (similaridade semÃ¢ntica)
    resultados = db.similarity_search_with_relevance_scores(pergunta)
    print(resultados)
    print(len(resultados))
    
    # ğŸ“„ Construir o contexto para o LLM
    contexto = ""
    for doc, score in resultados:
        contexto += f"\n{doc.page_content}"
    
    # ğŸ¤– Formatar o prompt completo
    prompt = ChatPromptTemplate.from_template(prompt_template)
    chain = prompt | ChatOpenAI()
    response = chain.invoke({
        "pergunta": pergunta,
        "base_conhecimento": contexto
    })
    
    print("Resposta da ia:")
    print(response.content)

# ğŸš€ Ponto de Entrada
if __name__ == "__main__":
    perguntar()
```

---

## ğŸ”§ Componentes Detalhados

### 1. ğŸ“š Imports e DependÃªncias

```python
from langchain_chroma.vectorstores import Chroma          # Banco vetorial
from langchain_openai import OpenAIEmbeddings             # GeraÃ§Ã£o de embeddings
from dotenv import load_dotenv                           # Carregar .env
from langchain_core.prompts import ChatPromptTemplate     # Templates de prompt
from langchain_openai import ChatOpenAI                  # Modelo LLM
```

**PropÃ³sito:** Importa todas as bibliotecas necessÃ¡rias para o pipeline RAG.

### 2. ğŸ”§ ConfiguraÃ§Ã£o Inicial

```python
load_dotenv()          # Carrega OPENAI_API_KEY do arquivo .env
CAMINHO_DB = "db"      # Define onde estÃ¡ o banco vetorial
```

**Por que importante:**
- `load_dotenv()` garante seguranÃ§a das API keys
- `CAMINHO_DB` permite flexibilidade de storage

### 3. ğŸ¤– Template de Prompt

```python
prompt_template = """VocÃª Ã© um assistente inteligente...
{pergunta}             # â† Placeholder para pergunta do usuÃ¡rio
{base_conhecimento}     # â† Placeholder para contexto recuperado
...
"""
```

**Elementos-chave:**
- **Persona**: "assistente inteligente" - Define comportamento
- **Contexto**: `{base_conhecimento}` - Onde o RAG insere documentos
- **Fallback**: "nÃ£o sei a resposta" - Controle de alucinaÃ§Ãµes
- **Linguagem**: PortuguÃªs - Alinhado com os documentos

### 4. ğŸ§  Banco Vetorial e Embeddings

```python
func_embedding = OpenAIEmbeddings()                              # Cria funÃ§Ã£o de embedding
db = Chroma(persist_directory=CAMINHO_DB, embedding_function=func_embedding)  # Carrega BD
```

**Como funciona:**
1. `OpenAIEmbeddings()` converte texto em vetores numÃ©ricos
2. `Chroma()` carrega o banco vetorial persistente do disco
3. O embedding function permite buscas semÃ¢nticas

---

## ğŸ” Fluxo de ExecuÃ§Ã£o Detalhado

### Etapa 1: ğŸ“ Entrada do UsuÃ¡rio
```python
pergunta = input("Digite sua pergunta: ")
```
- Captura pergunta em linguagem natural
- Exemplo: "O que Ã© heranÃ§a em Python?"

### Etapa 2: ğŸ§  Carregamento do Banco Vetorial
```python
func_embedding = OpenAIEmbeddings()
db = Chroma(persist_directory=CAMINHO_DB, embedding_function=func_embedding)
```
- Inicializa a funÃ§Ã£o de embedding (OpenAI)
- Carrega o ChromaDB do diretÃ³rio "db/"
- Verifica integridade do banco vetorial

### Etapa 3: ğŸ” Busca SemÃ¢ntica
```python
resultados = db.similarity_search_with_relevance_scores(pergunta)
```

**O que acontece internamente:**
1. **Embedding da Pergunta**: Pergunta â†’ Vetor numÃ©rico
2. **ComparaÃ§Ã£o Vetorial**: Vetor da pergunta vs todos os vetores do BD
3. **Ranking**: OrdenaÃ§Ã£o por similaridade (cosseno)
4. **SeleÃ§Ã£o**: Retorna os k mais similares (padrÃ£o: k=4)

**SaÃ­da esperada:**
```python
[
    (Document(page_content="HeranÃ§a Ã© um conceito...", metadata={'source': 'base/FAQ...pdf'}), 0.89),
    (Document(page_content="Classes filhas herdam...", metadata={'source': 'base/FAQ...pdf'}), 0.85),
    (Document(page_content="Em POO, heranÃ§a permite...", metadata={'source': 'base/FAQ...pdf'}), 0.78)
]
```

### Etapa 4: ğŸ“„ ConstruÃ§Ã£o do Contexto
```python
contexto = ""
for doc, score in resultados:
    contexto += f"\n{doc.page_content}"
```

**Processo:**
1. Extrai apenas o conteÃºdo textual dos documentos
2. Concatena todos os chunks recuperados
3. Cria uma base de conhecimento unificada

**Resultado final:**
```python
contexto = """
HeranÃ§a Ã© um dos pilares da programaÃ§Ã£o orientada a objetos...
Classes filhas herdam atributos e mÃ©todos das classes mÃ£es...
Em POO, heranÃ§a permite reutilizaÃ§Ã£o de cÃ³digo...
"""
```

### Etapa 5: ğŸ¤– FormataÃ§Ã£o e InvocaÃ§Ã£o do LLM
```python
prompt = ChatPromptTemplate.from_template(prompt_template)
chain = prompt | ChatOpenAI()
response = chain.invoke({
    "pergunta": pergunta,
    "base_conhecimento": contexto
})
```

**Pipeline LangChain:**
1. `ChatPromptTemplate.from_template()` â†’ Formata o prompt
2. `| ChatOpenAI()` â†’ Cria a chain do modelo
3. `.invoke()` â†’ Executa com os parÃ¢metros

**Prompt final enviado ao LLM:**
```python
"""VocÃª Ã© um assistente inteligente que ajuda os usuÃ¡rios com suas perguntas com base em documentos fornecidos:
O que Ã© heranÃ§a em Python?

Utilize as informaÃ§Ãµes do documentos para responder Ã  pergunta acima. ForneÃ§a respostas detalhadas e precisas:

HeranÃ§a Ã© um dos pilares da programaÃ§Ã£o orientada a objetos...
Classes filhas herdam atributos e mÃ©todos das classes mÃ£es...
Em POO, heranÃ§a permite reutilizaÃ§Ã£o de cÃ³digo...

Se a informaÃ§Ã£o nÃ£o estiver disponÃ­vel nos documentos, responda com "Desculpe, nÃ£o sei a resposta para essa pergunta."
"""
```

---

## âš™ï¸ ParÃ¢metros e ConfiguraÃ§Ãµes

### Constantes Globais
```python
CAMINHO_DB = "db"                    # DiretÃ³rio do ChromaDB
prompt_template = "..."              # Template do prompt
```

### VariÃ¡veis de Ambiente (.env)
```bash
OPENAI_API_KEY=sk-xxx               # Chave da API OpenAI
LLM_MODEL=gpt-3.5-turbo             # Modelo do LLM
EMBEDDING_MODEL=text-embedding-3-small  # Modelo de embedding
```

### ConfiguraÃ§Ãµes ImplÃ­citas
- **k=4**: NÃºmero padrÃ£o de documentos recuperados
- **score_threshold**: Sempre retorna os k mais similares
- **temperature**: Default do modelo (criatividade)

---

## ğŸ“ Exemplos de Uso

### Exemplo 1: Pergunta Direta
```bash
python main.py
Digite sua pergunta: O que Ã© Python?

# Sistema busca documentos sobre definiÃ§Ã£o de Python
# Retorna resposta baseada nos PDFs processados
```

### Exemplo 2: Pergunta Comparativa
```bash
Digite sua pergunta: Qual a diferenÃ§a entre lista e tupla?

# Busca por menÃ§Ãµes de ambos os termos
# Compara informaÃ§Ãµes dos documentos
```

### Exemplo 3: Pergunta PrÃ¡tica
```bash
Digite sua pergunta: Como instalar uma biblioteca?

# Procura por tutorias ou instruÃ§Ãµes nos documentos
# Responde com base no conteÃºdo encontrado
```

---

## ğŸš€ Melhorias e ExtensÃµes

### 1. Adicionar Filtros de RelevÃ¢ncia
```python
# Filtrar por score mÃ­nimo
resultados_filtrados = [(doc, score) for doc, score in resultados if score > 0.7]
```

### 2. Personalizar NÃºmero de Resultados
```python
# Buscar mais documentos para perguntas complexas
k = 6 if len(pergunta.split()) > 10 else 4
resultados = db.similarity_search_with_relevance_scores(pergunta, k=k)
```

### 3. Adicionar Metadata nos Resultados
```python
print(f"Fonte: {doc.metadata.get('source', 'Desconhecido')}")
print(f"Score: {score:.2f}")
```

---

## âš ï¸ Pontos de AtenÃ§Ã£o

### Requisitos ObrigatÃ³rios
- âœ… Arquivo `.env` configurado com `OPENAI_API_KEY`
- âœ… Banco vetorial criado em `db/` (executar `python db.py` primeiro)
- âœ… DependÃªncias instaladas (`pip install -r requirements.txt`)

### LimitaÃ§Ãµes Conhecidas
- **CrÃ©ditos API**: Cada consulta consome tokens da OpenAI
- **Tamanho do Contexto**: Muitos documentos podem exceder o limite
- **Performance**: LatÃªncia depende da API OpenAI

### Boas PrÃ¡ticas
- ğŸ’° Monitore consumo da API OpenAI
- ğŸ“Š Use scores de relevÃ¢ncia para filtrar resultados
- ğŸ¯ FaÃ§a perguntas especÃ­ficas para melhores respostas
- ğŸ”„ Limpe o cache do ChromaDB se atualizar documentos

---

## ğŸ”® PossÃ­veis Melhorias Futuras

1. **Interface Web**: Adicionar Streamlit ou Flask
2. **HistÃ³rico**: Salvar conversas anteriores
3. **Feedback**: Sistema de avaliaÃ§Ã£o de respostas
4. **Batch**: Processar mÃºltiplas perguntas
5. **Cache**: Respostas em cache para perguntas repetidas
6. **MÃ©tricas**: AnÃ¡lise de qualidade das respostas

---

## ğŸ“Š Resumo TÃ©cnico

| Componente | Tecnologia | FunÃ§Ã£o |
|------------|------------|--------|
| **Banco Vetorial** | ChromaDB | Armazenamento e busca de embeddings |
| **Embeddings** | OpenAI | ConversÃ£o texto â†’ vetor |
| **LLM** | GPT-3.5/4 | GeraÃ§Ã£o de respostas |
| **Template** | LangChain | FormataÃ§Ã£o de prompts |
| **Interface** | Terminal CLI | InteraÃ§Ã£o com usuÃ¡rio |

Este arquivo representa o coraÃ§Ã£o da interface RAG, conectando usuÃ¡rios a conhecimento especializado atravÃ©s de busca semÃ¢ntica e geraÃ§Ã£o aumentada.