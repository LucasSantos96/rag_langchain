import streamlit as st
import os
import sys
from dotenv import load_dotenv
import time
import json
from datetime import datetime

# Configura√ß√£o da p√°gina
st.set_page_config(
    page_title="ü§ñ Assistente RAG - Python Knowledge",
    page_icon="üêç",
    layout="centered",
    initial_sidebar_state="expanded",
)

# Carregar vari√°veis de ambiente
load_dotenv()

# Configura√ß√µes
CAMINHO_DB = "db"
PASTA_DOCUMENTOS = "base"

# Imports do projeto
try:
    from langchain_chroma.vectorstores import Chroma
    from langchain_openai import OpenAIEmbeddings
    from langchain_core.prompts import ChatPromptTemplate
    from langchain_groq import ChatGroq
except ImportError as e:
    st.error(f"‚ùå Erro ao importar bibliotecas: {e}")
    st.stop()

# Configura√ß√µes
CAMINHO_DB = "db"

# CSS para estiliza√ß√£o
st.markdown(
    """
<style>
    /* Header styles */
    .main-header {
        font-size: 2.5rem;
        font-weight: bold;
        text-align: center;
        color: #1f77b4;
        margin-bottom: 1rem;
    }
    .sub-header {
        font-size: 1.2rem;
        text-align: center;
        color: #666;
        margin-bottom: 2rem;
    }
    
    /* Chat message styles with dark text */
    .chat-message {
        padding: 1rem;
        border-radius: 0.5rem;
        margin-bottom: 1rem;
        color: #000000 !important;
    }
    .chat-message p, .chat-message span, .chat-message div {
        color: #000000 !important;
    }
    .user-message {
        background-color: #e3f2fd;
        border-left: 4px solid #2196f3;
        color: #000000 !important;
    }
    .user-message strong {
        color: #1565c0 !important;
    }
    .ai-message {
        background-color: #f3e5f5;
        border-left: 4px solid #9c27b0;
        color: #000000 !important;
    }
    .ai-message strong {
        color: #7b1fa2 !important;
    }
    
    /* Text input styles - ensure dark text */
    .stTextInput > div > div > input {
        background-color: #ffffff;
        color: #000000 !important;
        border: 2px solid #e0e0e0;
        font-size: 1rem;
    }
    .stTextInput > div > div > input::placeholder {
        color: #9e9e9e !important;
    }
    .stTextInput > label {
        color: #333333 !important;
        font-weight: 500;
    }
    
    /* Source and expander styles */
    .source-info {
        font-size: 0.85rem;
        color: #424242 !important;
        margin-top: 0.5rem;
    }
    .score-badge {
        background-color: #4caf50;
        color: white !important;
        padding: 0.2rem 0.5rem;
        border-radius: 0.3rem;
        font-size: 0.8rem;
        font-weight: bold;
    }
    
    /* Streamlit expander styles - ensure dark text */
    .streamlit-expanderHeader {
        color: #333333 !important;
        font-weight: 500;
    }
    .streamlit-expanderContent {
        color: #333333 !important;
    }
    
    /* General text colors */
    p, span, div, label {
        color: #333333;
    }
    
    /* Button styles */
    .stButton > button {
        font-weight: 500;
    }
    
    /* Caption styles */
    .stCaption {
        color: #666666 !important;
        font-size: 0.85rem;
    }
</style>
""",
    unsafe_allow_html=True,
)


# Fun√ß√£o para verificar se o banco de dados existe
@st.cache_resource(show_spinner=False)
def carregar_banco_dados():
    """Carrega o banco vetorial com cache para performance."""
    if not os.path.exists(CAMINHO_DB):
        st.error("‚ùå Banco de dados n√£o encontrado! Execute `python db.py` primeiro.")
        return None

    try:
        embeddings = OpenAIEmbeddings()
        db = Chroma(persist_directory=CAMINHO_DB, embedding_function=embeddings)
        return db
    except Exception as e:
        st.error(f"‚ùå Erro ao carregar banco de dados: {e}")
        return None


# Fun√ß√£o para verificar documentos dispon√≠veis
def verificar_documentos():
    """Verifica quais documentos PDF est√£o dispon√≠veis."""
    if not os.path.exists(PASTA_DOCUMENTOS):
        return []

    documentos = []
    for arquivo in os.listdir(PASTA_DOCUMENTOS):
        if arquivo.endswith(".pdf"):
            caminho_completo = os.path.join(PASTA_DOCUMENTOS, arquivo)
            tamanho_mb = os.path.getsize(caminho_completo) / (1024 * 1024)
            documentos.append(
                {
                    "nome": arquivo,
                    "tamanho": f"{tamanho_mb:.1f} MB",
                    "caminho": caminho_completo,
                }
            )

    return documentos


# Fun√ß√£o para buscar informa√ß√µes do sistema
def obter_info_sistema(db):
    """Obt√©m informa√ß√µes detalhadas sobre o banco de dados."""
    if db is None:
        return {}

    try:
        # Tenta obter informa√ß√µes da cole√ß√£o
        info = {}

        # Tamanho do banco
        if os.path.exists(f"{CAMINHO_DB}/chroma.sqlite3"):
            tamanho_bytes = os.path.getsize(f"{CAMINHO_DB}/chroma.sqlite3")
            info["tamanho_banco"] = f"{tamanho_bytes / (1024 * 1024):.1f} MB"

        # Documentos dispon√≠veis
        info["documentos"] = verificar_documentos()
        info["num_documentos"] = len(info["documentos"])

        # Configura√ß√µes de modelo
        info["embedding_model"] = os.getenv("EMBEDDING_MODEL", "text-embedding-3-small")
        info["llm_model"] = os.getenv("LLM_MODEL", "gpt-3.5-turbo")

        return info
    except Exception as e:
        st.error(f"Erro ao obter informa√ß√µes: {e}")
        return {}


# Fun√ß√£o para processar a pergunta
def processar_pergunta(pergunta, db, k_docs=4, temperature=0.1):
    """Processa uma pergunta usando o sistema RAG com par√¢metros configur√°veis."""
    try:
        start_time = time.time()

        # Mostrar spinner durante processamento
        with st.spinner("üîç Buscando informa√ß√µes no banco de conhecimento..."):
            # Buscar documentos similares
            resultados = db.similarity_search_with_relevance_scores(pergunta, k=k_docs)

        if not resultados:
            return (
                "N√£o encontrei informa√ß√µes relevantes para sua pergunta nos documentos dispon√≠veis.",
                [],
                0.0,
            )

        # Construir contexto
        contexto = ""
        fontes = []

        for i, (doc, score) in enumerate(resultados):
            contexto += f"\nüìÑ **Documento {i + 1}** (Similaridade: {score:.3f}):\n{doc.page_content}\n"
            fontes.append(
                {
                    "conteudo": doc.page_content[:300] + "...",
                    "fonte": doc.metadata.get("source", "Desconhecido"),
                    "score": score,
                    "pagina": doc.metadata.get("page", "N/A"),
                    "chunk_id": i + 1,
                }
            )

        # Gerar resposta com LLM
        with st.spinner("ü§ñ Gerando resposta com IA..."):
            prompt_template = """Voc√™ √© um assistente inteligente especialista em Python que ajuda os usu√°rios com suas perguntas com base nos documentos fornecidos.

**PERGUNTA DO USU√ÅRIO:**
{pergunta}

**CONTEXTO DISPON√çVEL (Base de Conhecimento):**
{base_conhecimento}

**INSTRU√á√ïES ESPEC√çFICAS:**
1. Responda APENAS com base nas informa√ß√µes fornecidas nos documentos acima
2. Se a informa√ß√£o n√£o estiver dispon√≠vel, responda claramente: "Desculpe, n√£o encontrei essa informa√ß√£o espec√≠fica nos documentos dispon√≠veis."
3. Seja claro, direto e educativo na sua resposta
4. Use exemplos pr√°ticos quando os documentos fornecerem
5. Estruture sua resposta em par√°grafos curtos e f√°ceis de ler
6. Seja honesto sobre as limita√ß√µes do conhecimento dispon√≠vel

**RESPOSTA:**"""

            prompt = ChatPromptTemplate.from_template(prompt_template)

            # Configurar modelo com GROQ (gratuito e ultra-r√°pido)
            groq_api_key = os.getenv("GROQ_API_KEY")
            groq_model = os.getenv("GROQ_MODEL", "llama-3.1-8b-instant")

            if not groq_api_key:
                return "‚ùå Erro: GROQ_API_KEY n√£o configurada no arquivo .env", [], 0.0

            chat = ChatGroq(
                temperature=temperature,
                model=groq_model,
                api_key=groq_api_key,
            )
            chain = prompt | chat
            response = chain.invoke(
                {"pergunta": pergunta, "base_conhecimento": contexto}
            )

        processing_time = time.time() - start_time
        return response.content, fontes, processing_time

    except Exception as e:
        return f"‚ùå Erro ao processar pergunta: {str(e)}", [], 0.0


def verificar_banco_dados():
    """Retorna (True, db) se o banco estiver carregado, caso contr√°rio (False, None)."""
    db = carregar_banco_dados()
    if db is None:
        return False, None
    return True, db


# Fun√ß√£o principal
def main():
    # Header
    st.markdown(
        '<div class="main-header">ü§ñ Assistente RAG</div>', unsafe_allow_html=True
    )
    st.markdown(
        '<div class="sub-header">Consulte o conhecimento sobre Python com IA</div>',
        unsafe_allow_html=True,
    )

    # Sidebar com informa√ß√µes
    with st.sidebar:
        st.header("üìä Informa√ß√µes do Sistema")

        # Verificar API Key
        api_key = os.getenv("OPENAI_API_KEY")
        if api_key:
            st.success("‚úÖ API Key configurada")
            st.write(f"**Key:** {api_key[:10]}...")
        else:
            st.error("‚ùå API Key n√£o encontrada")
            st.info("Configure a vari√°vel OPENAI_API_KEY no arquivo .env")

        st.divider()

        # Status do banco de dados
        st.header("üóÑÔ∏è Banco de Dados")
        if os.path.exists(CAMINHO_DB):
            st.success("‚úÖ Banco de dados encontrado")

            # Tentar carregar e mostrar informa√ß√µes
            try:
                embeddings = OpenAIEmbeddings()
                db = Chroma(persist_directory=CAMINHO_DB, embedding_function=embeddings)

                # Contar documentos (se poss√≠vel)
                st.info("üìä Banco de dados carregado com sucesso")

            except Exception as e:
                st.error(f"‚ùå Erro ao carregar banco: {e}")
        else:
            st.error("‚ùå Banco de dados n√£o encontrado")
            st.code("python db.py")

        st.divider()

        # Informa√ß√µes do modelo
        st.header("ü§ñ Modelo de Chat")
        groq_model = os.getenv("GROQ_MODEL", "llama-3.1-8b-instant")

        st.success(f"‚úÖ {groq_model}")
        st.write("üöÄ Groq - Infer√™ncia Ultra-R√°pida")
        st.write("üÜì Modelo Gratuito")
        st.write("‚ö° Respostas em milissegundos")

        if groq_api_key := os.getenv("GROQ_API_KEY"):
            st.caption(f"üîë API Key: {groq_api_key[:10]}...")

        st.divider()

        # Informa√ß√µes do projeto
        st.header("‚ÑπÔ∏è Sobre o Projeto")
        st.write("""
        **Sistema RAG com LangChain**
        
        - üìö Baseado em documentos PDF
        - üß† Busca sem√¢ntica avan√ßada
        - ü§ñ Respostas geradas por IA
        - üîç Fontes sempre citadas
        """)

        st.divider()

        # Link do projeto
        st.header("üîó Links")
        st.markdown("""
        [üìÇ Reposit√≥rio GitHub](https://github.com/LucasSantos96/rag_langchain)
        
        [üé• V√≠deo Tutorial](https://www.youtube.com/watch?v=0M8iO5ykY-E)
        """)

        st.divider()

        # Estat√≠sticas
        st.header("üìà Como Funciona")
        st.write("""
        1. üìÑ Sua pergunta √© convertida em embedding
        2. üîç Busca nos documentos por similaridade
        3. üìÑ Contexto relevante √© recuperado
        4. ü§ñ IA gera resposta baseada no contexto
        """)

    # √Årea principal
    st.header("üí¨ Fa√ßa sua Pergunta")

    # Input da pergunta
    col1, col2 = st.columns([4, 1])

    with col1:
        pergunta = st.text_input(
            "Digite sua pergunta sobre Python:",
            placeholder="Ex: O que √© programa√ß√£o orientada a objetos?",
            key="input_pergunta",
        )

    with col2:
        st.write("")
        st.write("")
        if st.button("üîç Perguntar", type="primary"):
            if not pergunta.strip():
                st.warning("‚ö†Ô∏è Por favor, digite uma pergunta.")
            else:
                # Verificar banco de dados
                resultado, db = verificar_banco_dados()
                if resultado:
                    # Processar pergunta
                    resposta, fontes, processing_time = processar_pergunta(pergunta, db)

                    # Salvar no hist√≥rico
                    if "historico" not in st.session_state:
                        st.session_state.historico = []

                    st.session_state.historico.append(
                        {
                            "pergunta": pergunta,
                            "resposta": resposta,
                            "fontes": fontes,
                            "timestamp": time.time(),
                            "processing_time": processing_time,
                        }
                    )

    # Exibir hist√≥rico
    if "historico" in st.session_state and st.session_state.historico:
        st.divider()
        st.header("üìú Hist√≥rico de Conversas")

        # Exibir em ordem reversa (mais recente primeiro)
        for i, conversa in enumerate(reversed(st.session_state.historico)):
            with st.container():
                # Pergunta do usu√°rio
                st.markdown(
                    f"""
                <div class="chat-message user-message">
                    <strong>üë§ Voc√™:</strong> {conversa["pergunta"]}
                </div>
                """,
                    unsafe_allow_html=True,
                )

                # Resposta da IA
                st.markdown(
                    f"""
                <div class="chat-message ai-message">
                    <strong>ü§ñ Assistente:</strong> {conversa["resposta"]}
                </div>
                """,
                    unsafe_allow_html=True,
                )

                # Fontes e scores
                if conversa["fontes"]:
                    with st.expander(
                        f"üìö Fontes e Relev√¢ncia ({len(conversa['fontes'])} documentos)"
                    ):
                        for j, fonte in enumerate(conversa["fontes"]):
                            st.markdown(
                                f"""
                            <div style="border-left: 3px solid #2196f3; padding-left: 1rem; margin-bottom: 0.5rem;">
                                <strong>Fonte {j + 1}:</strong> {fonte["fonte"]}
                                <span class="score-badge">Score: {fonte["score"]:.3f}</span>
                                <br>
                                <em>{fonte["conteudo"]}</em>
                            </div>
                            """,
                                unsafe_allow_html=True,
                            )

                # Tempo de processamento
                if "processing_time" in conversa:
                    st.caption(f"‚ö° Processado em {conversa['processing_time']:.2f}s")

                st.divider()

    # Limpar hist√≥rico
    if "historico" in st.session_state and st.session_state.historico:
        if st.button("üóëÔ∏è Limpar Hist√≥rico"):
            st.session_state.historico = []
            st.rerun()

    # Footer
    st.markdown("---")
    st.markdown(
        """
    <div style="text-align: center; color: #666; font-size: 0.9rem;">
        ü§ñ Powered by <strong>LangChain + Groq (Llama 3.1) + ChromaDB</strong> | 
        üìö Projeto RAG Completo | 
        üîó <a href="https://github.com/LucasSantos96/rag_langchain" target="_blank">GitHub</a>
    </div>
    """,
        unsafe_allow_html=True,
    )


if __name__ == "__main__":
    main()
